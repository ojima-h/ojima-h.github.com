
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>毛無しさん@キレートレモン</title>
  <meta name="author" content="ojima_h">

  
  <meta name="description" content="Hadoop本
HADOOP HACKS を参考に、HiveQL が どんな Map/Reduce タスクに展開されるのかを想像しつつ(ソースは読んでないのであくまで想像)、
効率の良い Hiveクエリの書き方を考えてみる。 まずは、普通のクエリ SELECT * FROM movie は、 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ojima-h.github.com/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="毛無しさん@キレートレモン" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-35619154-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">毛無しさん@キレートレモン</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:ojima-h.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/08/30/hive-optimization/">Hive クエリを最適化する</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-08-30T11:23:00+09:00" pubdate data-updated="true">Aug 30<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><ul>
<li><a href="http://www.amazon.co.jp/Hadoop-Tom-White/dp/487311439X">Hadoop本</a></li>
<li><a href="http://www.amazon.co.jp/Hadoop-Hacks-%E2%80%95%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%81%8C%E4%BD%BF%E3%81%86%E5%AE%9F%E8%B7%B5%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF-%E4%B8%AD%E9%87%8E-%E7%8C%9B/dp/4873115469">HADOOP HACKS</a></li>
</ul>


<p>を参考に、HiveQL が どんな Map/Reduce タスクに展開されるのかを想像しつつ(ソースは読んでないのであくまで想像)、
効率の良い Hiveクエリの書き方を考えてみる。</p>

<h2>まずは、普通のクエリ</h2>

<pre><code>SELECT * FROM movie
</code></pre>

<p>は、どんな Map/Reduce タスクに変換されるんでしょうか？</p>

<p>hive で</p>

<pre><code>&gt; EXPLAIN SELECT * FROM movie;
</code></pre>

<p>とやってみると、</p>

<pre><code>ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME movie))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))))

STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
</code></pre>

<p>と返っくる。
たぶん、<code>Stage-0</code> は「ただ吐き出す」ていうタスクなんだろう。</p>

<p><code>SELECT * FROM movie LIMIT 10</code> と比較してみる:</p>

<pre><code>&gt; EXPLAIN SELECT * FROM movie LIMIT 10;

ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME all_member))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_LIMIT 10)))

STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: 10
</code></pre>

<p><code>Stage-0</code> は、<code>LIMIT</code> の面倒だけ見ている気がする。</p>

<h2>SELECT WHERE してみる</h2>

<pre><code>&gt; EXPLAIN SELECT id FROM movie WHERE year = 2000 LIMIT 10;

ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME movie))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL id))) (TOK_WHERE (= (TOK_TABLE_OR_COL year) 2000))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&gt; Map Operator Tree:
        movie 
          TableScan
            alias: movie
            Filter Operator
              predicate:
                  expr: (year = 2000)
                  type: boolean
              Select Operator
                expressions:
                      expr: id
                      type: int
                outputColumnNames: _col0
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1
</code></pre>

<p>たぶん mapper はこんなイメージかな&#8230;？</p>

<pre><code>public void map(Integer key, Text row, OutputCollector&lt;Integer, Text&gt; output, ...) {
    Integer id = row.getId();
    Integer year = row.getYear();

    if (year == 2000) {
        output.collect(key, new Writable(id));
    }
}
</code></pre>

<h2>GROUP BY してみる</h2>

<pre><code>&gt; EXPLAIN SELECT year, count(*) FROM movie GROUP BY year;

ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME movie))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL year)) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_GROUPBY (TOK_TABLE_OR_COL year))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&gt; Map Operator Tree:
        movie 
          TableScan
            alias: movie
            Select Operator
              expressions:
                    expr: year
                    type: string
              outputColumnNames: year
              Group By Operator
                aggregations:
                      expr: count()
                bucketGroup: false
                keys:
                      expr: year
                      type: string
                mode: hash
                outputColumnNames: _col0, _col1
                Reduce Output Operator
                  key expressions:
                        expr: _col0
                        type: string
                  sort order: +
                  Map-reduce partition columns:
                        expr: _col0
                        type: string
                  tag: -1
                  value expressions:
                        expr: _col1
                        type: bigint
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: count(VALUE._col0)
          bucketGroup: false
          keys:
                expr: KEY._col0
                type: string
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Select Operator
            expressions:
                  expr: _col0
                  type: string
                  expr: _col1
                  type: bigint
            outputColumnNames: _col0, _col1
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1
</code></pre>

<p>Map Operator Tree の中に Reduce Output Operator があるのは、map task と reduce task 間のデータ転送量を小さくするために
map task 側で集約関数(combiner)を呼ぶためだと思われる。</p>

<p>たぶん mapper はこんなイメージ&#8230;</p>

<pre><code>public void map(Integer key, Text row, OutputCollector&lt;Integer, Text&gt; output, ...) {
    Integer year = row.getYear();

    output.collect(year, new Writable(key));
}
</code></pre>

<p>reducer (と combiner) は多分こんな感じ&#8230;</p>

<pre><code>publick void reduce(Text year, Iterator&lt;Text&gt; values, OutputCollector&lt;Text,BigInt&gt; output, ...) {
    BigInt count = values.count();

    output.collect(&lt;何かしらのKEY&gt;, new Writable(new Row(year, count)));
}
</code></pre>

<h2>問題は JOIN</h2>

<pre><code>&gt; EXPLAIN SELECT a.id, b.rating FROM movie a JOIN score b on a.id = b.movie_id

ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME movie) a) (TOK_TABREF (TOK_TABNAME score) b) (= (. (TOK_TABLE_OR_COL a) id) (. (TOK_TABLE_OR_COL b) movie_id)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) id)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) rating)))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&gt; Map Operator Tree:
        a 
          TableScan
            alias: a
            Reduce Output Operator
              key expressions:
                    expr: id
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: id
                    type: int
              tag: 0
              value expressions:
                    expr: id
                    type: int
        b 
          TableScan
            alias: b
            Reduce Output Operator
              key expressions:
                    expr: movie_id
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: movie_id
                    type: int
              tag: 1
              value expressions:
                    expr: rating
                    type: int
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          condition expressions:
            0 {VALUE._col0}
            1 {VALUE._col1}
          handleSkewJoin: false
          outputColumnNames: _col0, _col3
          Select Operator
            expressions:
                  expr: _col0
                  type: int
                  expr: _col3
                  type: int
            outputColumnNames: _col0, _col1
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1
</code></pre>

<p>処理の流れはたぶんこんな感じ:</p>

<ol>
<li><p><strong>MAPPER</strong></p>

<p><code>movie</code> テーブルと <code>score</code> テーブルから必要なカラムを抜きだす。</p></li>
<li><p><strong>REDUCER</strong></p>

<p>mapper からデータをコピーする。このとき、同じ結合キー (movie.id と score.movie_id) を持つデータは
同じ reduce task にコピーされる。
で、結合して、指定されたカラムを返す。</p></li>
</ol>


<h2>MAPJOIN してみる</h2>

<p>HADOOP HACKS とかに、マップサイドジョンが紹介されていたので、試してみる。</p>

<pre><code>&gt; EXPLAIN SELECT /*+ MAPJOIN(a) */ id, rating FROM movie a JOIN score b ON a.id = b.movie_id;

ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME movie) a) (TOK_TABREF (TOK_TABNAME score) b) (= (. (TOK_TABLE_OR_CO
L a) id) (. (TOK_TABLE_OR_COL b) movie_id)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TO
K_MAPJOIN (TOK_HINTARGLIST a))) (TOK_SELEXPR (TOK_TABLE_OR_COL id)) (TOK_SELEXPR (TOK_TABLE_OR_COL rating)))))

STAGE DEPENDENCIES:
  Stage-3 is a root stage
  Stage-1 depends on stages: Stage-3
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-3
    Map Reduce Local Work
      Alias -&gt; Map Local Tables:
        a 
          Fetch Operator
            limit: -1
      Alias -&gt; Map Local Operator Tree:
        a 
          TableScan
            alias: a
            HashTable Sink Operator
              condition expressions:
                0 {id}
                1 {rating}
              handleSkewJoin: false
              keys:
                0 [Column[id]]
                1 [Column[movie_id]]
              Position of Big Table: 1

  Stage: Stage-1
    Map Reduce
      Alias -&gt; Map Operator Tree:
        b 
          TableScan
            alias: b
            Map Join Operator
              condition map:
                   Inner Join 0 to 1
              condition expressions:
                0 {id}
                1 {rating}
              handleSkewJoin: false
              keys:
                0 [Column[id]]
                1 [Column[movie_id]]
              outputColumnNames: _col0, _col3
              Position of Big Table: 1
              Select Operator
                expressions:
                      expr: _col0
                      type: int
                      expr: _col3
                      type: int
                outputColumnNames: _col0, _col3
                Select Operator
                  expressions:
                        expr: _col0
                        type: int
                        expr: _col3
                        type: int
                  outputColumnNames: _col0, _col1
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
      Local Work:
        Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
</code></pre>

<p>たしかに reduce task がなくなっている。</p>

<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization</a> には、MAPJOIN の説明が以下のようにされている。</p>

<blockquote><p>  MAPJOINs are processed by loading the smaller table into an in-memory hash map and matching keys with the larger table as they are streamed through. The prior implementation has this division of labor:</p>

<ul>
<li>  Local work:

<ul>
<li>  read records via standard table scan (including filters and projections) from source on local machine</li>
<li>  build hashtable in memory</li>
<li>  write hashtable to local disk</li>
<li>  upload hashtable to dfs</li>
<li>  add hashtable to distributed cache</li>
</ul>
</li>
<li>  Map task

<ul>
<li>  read hashtable from local disk (distributed cache) into memory</li>
<li>  match records&#8217; keys against hashtable</li>
<li>  combine matches and write to output</li>
</ul>
</li>
<li>  No reduce task</li>
</ul>
</blockquote>

<p>Stage-3 で、<code>movie</code> テーブルを読み込んで hashtable を構築しているようだ。
で、Stage-1 の map task で、この hashtable を使いながら <code>socre</code> テーブルを読み込んでいくのだろう。</p>

<h2>Map Side Join についてもう少し考えてみる</h2>

<p>Hadoop本の中で、「map側結合」が紹介されていた。(p.254 8.3.1 map側結合)
ここで説明されている map 側結合と、Hive の MAPJOIN はどうも違うようだ。</p>

<p>Hadoop本の map 側結合が Hive クエリ実行時に行なわれることはないのだろうか？
Join 対象になる2つのテーブル (もしくは、サブクエリの結果) のデータが、
結合キーごとに同じデータノードに入っている状況。</p>

<p>うん。。</p>

<p>思いつかない。</p>

<h2>その他の JOIN の最適化方法</h2>

<ul>
<li><a href="https://cwiki.apache.org/confluence/download/attachments/27362054/Hive+Summit+2011-join.pdf?version=1&amp;modificationDate=1310001042000">https://cwiki.apache.org/confluence/download/attachments/27362054/Hive+Summit+2011-join.pdf?version=1&amp;modificationDate=1310001042000</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization">https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization</a></li>
</ul>


<p>とかもあるみたい。</p>

<h3>Bucket Map Join</h3>

<p>Join 対象のテーブルが bucketize されている場合、構築された hashtable を bucket key に基づいて、必要な map task にだけ配布するっぽい。</p>

<h3>Sort Merge Map Join</h3>

<p>Join 対象のテーブルが bucketize されていて、しかも sort されている場合、もっと早くなるみたい。</p>

<p>でも、sort するのが大変。</p>

<h3>Skewed Join</h3>

<p>よく分からない。</p>

<h2>その他、思ったこと</h2>

<ul>
<li><p>MAPJOIN はテーブル1つをまるごとメモリに載せないといけないので、
せいぜい数十メガから数百メガのテーブルじゃないと辛そう。
サブクエリの中で十分絞り込んだ後 JOIN するのが実用的かも。</p></li>
<li><p>Bucket Map Join は、既存のHiveテーブルに対して、後から bucketize するのは大変そう。
集計用の中間テーブルを作ることがあったら、検討してみるといいかも。</p></li>
<li><p>HADOOP HACKS の中で、UDF (User Define Fucntion) や UDAF (User Define Aggregate Function) の作り方が載っていた。
この辺をもっとサクサク使えるようになるとはかどりそうな気がした。
むしろ、使わないともったいない。</p></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/08/29/hadoop-tuning/">Hadoop のチューニングに関してまとめ</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-08-29T16:06:00+09:00" pubdate data-updated="true">Aug 29<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>1効率の良い HiveQL を書くときに気をつけるべきことをまとめてみる。</p>

<p><em>以下に書いたことは、ほとんど全部自分なりの解釈なので、間違いが多々あると思います</em></p>

<p>Hadoop一般のチューニングに関しては、ここを参考にした。</p>

<p><a href="http://www.cloudera.co.jp/jpevents/cloudera-world-tokyo/pdf/A3_Hadoop%E3%81%AE%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E8%A8%AD%E8%A8%88_%E9%81%8B%E7%94%A8%E3%81%AE%E3%83%9D%E3%82%A4%E3%83%B3%E3%83%88.pdf">http://www.cloudera.co.jp/jpevents/cloudera-world-tokyo/pdf/A3_Hadoop%E3%81%AE%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E8%A8%AD%E8%A8%88_%E9%81%8B%E7%94%A8%E3%81%AE%E3%83%9D%E3%82%A4%E3%83%B3%E3%83%88.pdf</a></p>

<p><a href="http://www.amazon.co.jp/Hadoop-Tom-White/dp/487311439X">Hadoop本</a> を参考に、この資料の補足してみた↓</p>

<h2>ラック内ネットワークとラック間ネットワークは何が違うのか？</h2>

<p><em>(p.69 「ネットワークトポロジとHadoop」 / p.267 9.1.1 ネットワークトポロジ)</em></p>

<p>Hadoop は、データを読み書きする際にネットワークのトポロジやネットワーク構成を勘案しながら動作する。
読み込みを行う際には、クライアントになるべく <em>近い</em> ノードから読み込むように調整するし、
書き込みを行う際には、データのレプリカが近い場所に集中してしまわないように調整する。</p>

<p>2つのノード間のデータ転送速度が早いほど、この2つのノードが <em>近い</em> と見做される。
Hadoop は、ノード間の距離をネットワーク構成から算出している。
つまり、「同じラックにある2つのノードは互いに近い = 転送速度が早い」ものとして処理を進める。</p>

<p>そのため、同じラック内にあるのに実際には転送速度が遅い、ということになると、
Hadoop が効率良くデータを処理することができなくなってしまう。</p>

<h2>スレーブノードでは、なぜ RAID0 を組んではダメなのか？</h2>

<p><em>(p.266 「RAIDを使わないのはなぜ？」)</em></p>

<ul>
<li>HDFSはノード間の複製によって冗長性を持つので、RAIDで冗長性を提供してもらう必要がない。(←弱い)</li>
<li>RAID0 でデータロストが発生した事例あり</li>
</ul>


<h2>マスターノードでRaidを組まないと何が起きるのか？</h2>

<p>マスターノード (= ネームノード) は、クラスタ上のどのノードにどのファイルのデータが保存されているかを管理している。
マスターノードのデータが壊れると、HDFSからファイルデータを読み取ることができなくなってしまう。</p>

<p>そのため、マスターノードは信頼性を確保しておく必要がある。</p>

<h2>マスターとスレーブで、求められるCPU性能が違うのはなぜ？</h2>

<p>実際に計算するのはスレーブノード。
マスターノードは、クライアントの要求を受けて適切なスレーブノードに指令を出すのが仕事。
だから、マスターノードはたいしてCPU性能を必要としない。</p>

<h2>ECCとはなんぞ？</h2>

<p><a href="http://e-words.jp/w/ECCE383A1E383A2E383AA.html">Error Check and Correct memory</a></p>

<blockquote><p>メモリに誤った値が記録されていることを検出し、正しい値に訂正することができるメモリモジュール。</p></blockquote>

<h2>なんで圧縮したら「速度」があがるの？</h2>

<p>マップタスクからの出力は一時的にディスクに保存される。
レデュースタスクからの出力もディスクに保存される。</p>

<p>一般的には、Hadoopはかなり大きなデータを扱うので、出力結果を圧縮しないままディスクに書き込む時間よりも、
(圧縮する時間 + 圧縮データをディスクに書き込む時間) の方が短くなる。
読み込む時も同様に、非圧縮のデータを読み出す時間よりも、(圧縮されたデータを読み出す時間 + 解凍する時間) の方が短かくなる。</p>

<h2>ブロック数(サイズ)とヒープサイズの関係は？ (16枚目)</h2>

<p>ブロック数が小さい = ブロックサイズが大きい = ディスクから1回に読み込むデータのサイズが大きい = ディスクへのアクセスが減る</p>

<p>ヒープサイズが大きい = 計算結果を沢山メモリに置いておける = 計算結果をディスクに書き出すことが減る = ディスクへのアクセスが減る</p>

<p>⇒ <strong>速い</strong></p>

<h2>フェデレーション？て何？</h2>

<p><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html</a></p>

<blockquote><p>The prior HDFS architecture allows only a single namespace for the entire cluster. A single Namenode manages this namespace. HDFS Federation addresses limitation of the prior architecture by adding support multiple Namenodes/namespaces to HDFS file system.</p>

<p>In order to scale the name service horizontally, federation uses multiple independent Namenodes/namespaces. The Namenodes are federated, that is, the Namenodes are independent and don’t require coordination with each other. The datanodes are used as common storage for blocks by all the Namenodes. Each datanode registers with all the Namenodes in the cluster. Datanodes send periodic heartbeats and block reports and handles commands from the Namenodes.</p></blockquote>

<p>ネームノードの性能をスケールさせるために、HDFSファイル空間を2つに分けてしまって2つのネームノードでそれぞれ担当しよう、という話のようだ。</p>

<p>ネームノードの冗長化とはまた別の話なのかな？</p>

<h2>「ディスクのマウントポイントごとに別々のディレクトリをカンマ区切りで指定すること」てどういう意味？(21枚目)</h2>

<p>1つのデータノードのストレージはいくつかのディスクで構成されている。
ディスクが複数あってそれぞれにデータが分散して保存されているから、
データノードは処理を並列して行うことができる。</p>

<p>で、データノードは <code>dfs.datanode.data.dir</code> に指定されたディレクトリに対してラウンドロビンで書き込んでいくから、
<code>dfs.datanode.data.dir</code> には、複数あるディスクのそれぞれのマウントポイントを指定しないとダメだよ、ということ。</p>

<p>ちなみに、JBOD っていうのは、<a href="http://ja.wikipedia.org/wiki/JBOD">http://ja.wikipedia.org/wiki/JBOD</a> 参照。</p>

<h2>「タクク数 > ディスク数」てどういう状態？(25枚目)</h2>

<p>それぞれのタスクは、(普通は)ディスクからデータを読みとってから計算をはじめる。
「タクク数 > ディスク数」になってしまうと、並行してディスクアクセスを行なえるスレッド数(プロセス数？)の限界を越えて、
タスクノードがディスクアクセスを要求してくることになる。</p>

<p>で、ディスクアクセスが詰まってしまう。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/26/activerecord-number-1/">ActiveRecord #1</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-26T08:40:00+09:00" pubdate data-updated="true">Oct 26<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>前回 Arel が SQL を構築するところを読んだので、次は ActiveRecord がデータベースを操作するときに、どのタイミングで SQL を構築しているのかを見たいと思います。</p>

<p>このサンプルで、<code>User.where(...)</code>の時点でSQLが発行されてしまうと、その後に続く<code>.limit(10)</code>が反映されないはずじゃないか、というのか今回の疑問です。</p>

<hr />

<p>まず、<code>#where</code>の場所を探します。</p>

<p><code>ActiveRecord</code>はざっくりこんなかんじになっています↓</p>

<h2>Module#delegate</h2>

<p>ref. <a href="http://api.rubyonrails.org/classes/Module.html#method-i-delegate">http://api.rubyonrails.org/classes/Module.html#method-i-delegate</a></p>

<p><code>delegate</code> は、<code>active_support</code>の中で<code>Module#delegate</code>として定義されています。</p>

<p>上の例では、<code>Hoge#method</code>の呼び出すと、<code>Hoge#target</code>が返すオブジェクトの<code>method</code>メソッドが呼ばれます。</p>

<h2><code>Concern#included</code></h2>

<p><code>included</code>メソッドも同じく<code>active_support</code>で定義されており、
そのモジュールがincludeされた時に、ブロックの中身が実行されます。</p>

<hr />

<p>というわけで、<code>Relation#where</code>が呼び出されることが分かります。</p>

<h2><code>Relation#where</code></h2>

<p><code>build_where</code>の中身を追っていくと、
<code>PredicateBuilder#build_from_hash</code>内の以下のコードが実行されることが分かります。</p>

<p>上のコード内の<code>default_table</code>には
<code>Arel::Table</code>のインスタンスが入っています。</p>

<p>とういわけで、<code>User.where(:name =&gt; "John")</code>を実行したとき、
<code>Relation#build_where</code>の結果は<code>[table[:name].eq("John")]</code>となります。
これが<code>Relation#where_values</code>に格納されます。</p>

<p>以上で、<code>ActiveRecord::Base#where</code>の呼び出しは終わりです。</p>

<p>まとめると、<code>ActiveRecord::Base#where</code>を呼び出すと、<code>Relation</code>クラスのインスタンスが返されます。
その中には<code>table[column].eq(value)</code>たちが入った配列が格納されています。</p>

<h2><code>Relation#limit</code></h2>

<p><code>Relation#limit</code>も同じかんじです。</p>

<h2>で、いつSQL？</h2>

<p>さて、<code>User.where(..).limit(10)</code>の結果、<code>Relation</code>クラスのインスタンスが返ってくることは分かりましたが、
これがいつSQLに変換され、データベースに向けて投げられるんでしょうか？</p>

<p><code>where</code>の検索結果が必要になるのはどんな時でしょうか？</p>

<p>大抵の場合、検索結果から個々の<code>User</code>クラスのインスタンスを取り出すときかと思います。
このとき、検索結果に対しては Array としてアクセスすると思います。</p>

<p>というわけで、<code>Relation#to_a</code>について見てみました。</p>

<p><code>scope</code>とか<code>eager_loading</code>とかよく分からないことは置いておいて、デバッガで追いかけると、
<code>@klass.find_by_sql(arel, @bind_values)</code>が実行されることが分かります。</p>

<p>ということは、<code>arel</code>の値が問題になりそうです。</p>

<p>こんなかんじになってました。
この<code>build_arel</code>で、<code>where</code>とか<code>limit</code>に渡された引数たちを、
まとめて<code>Arel</code>のオブジェクトに変換していってます。</p>

<p><code>arel.xxx</code>が連なってる感じ、壮観です。</p>

<h2><code>find_by_sql</code></h2>

<p>この時点では<code>arel</code>はSQLになっていません。</p>

<p>なので、<code>find_by_sql</code>の中を追っていくことにします。</p>

<p>追っていくと、
<code>ActiveRecord::ConnectionAdapters::DatabaseStatements#select_all</code>
に行き着きます。</p>

<p><code>DatabaseManager#to_sql</code>の中の
<code>visitor.accept(arel.ast) do ...</code> というのが、
<code>Arel::TreeManger#to_sql</code>と同じことをやっているのが分かります。</p>

<p>というわけで、ここに来て、Arelのオブジェクトが無事SQLに変換されて、
DBMに向けて投げられたことが分かりました。</p>

<p>めでたし。</p>

<h2><code>to_a</code>は誰が呼ぶ？</h2>

<p><code>to_a</code>を呼び出せば、SQLが発行されることは分かりましたが、
誰がどのタイミングで<code>to_a</code>を呼ぶのでしょうか？</p>

<p>そういえば、<code>where</code>の検索結果に対しては、<code>each</code>メソッドを呼べます。</p>

<p><code>each</code>がどこで定義されているのか調べてみました。</p>

<p>なるほど。
<code>each</code>とか<code>map</code>などが呼ばれたタイミングで、<code>to_a</code>にデリゲートされると。</p>

<h2><code>Relation#inspect</code></h2>

<p>では、たとえば、irbの中で、<code>&gt; p User.where(...)</code>などとしたときはどうなんでしょうか？</p>

<p>このときも<code>to_a</code>が呼ばれるのでしょうか？</p>

<p><a href="http://doc.ruby-lang.org/ja/1.9.2/class/Object.html">http://doc.ruby-lang.org/ja/1.9.2/class/Object.html</a>によると↓とのこ
とです。</p>

<blockquote><p>  inspect -> String</p>

<pre><code>  オブジェクトを人間が読める形式に変換した文字列を返します。

  組み込み関数 Kernel.#p は、このメソッドの結果を使用して オブジェクトを表示します。
</code></pre></blockquote>

<p><code>p</code>は<code>inspect</code>の結果を表示するようです。</p>

<p><code>Relation#insepct</code>は</p>

<p>となってます。</p>

<p>やっぱり、<code>to_a</code>が呼ばれるようです。</p>

<h2>まとめ</h2>

<p>Arelのときもそうでしたが、
<code>where</code>などのメソッドが呼ばれた時点では単に引数だけを保存しておいて、
必要になった時点でSQLを発行するということでした。</p>

<p>「必要になった時点」の判断の仕方が面白いと思いました。</p>

<p>必要になった時点で<code>to_a</code>を呼ぶわけですが、<code>each</code>から<code>to_a</code>にdelegateされるとか、
<code>inspect</code>の中で<code>to_a</code>を呼ぶとか。</p>

<p>しかし、<code>where</code>とかをわざわざActiveRecordでラップする必要があるんですかね&#8230;
<code>Relation</code>のなかでArelのオブジェクトを保持しておいて、<code>where</code>の処理はArelにまかせて、
<code>Relation</code>は<code>to_a</code>とかのまわりだけ面倒みたらだめなのかな&#8230;？</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/19/linux-from-scratch-number-0/">Linux From Scratch 始めます</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-19T18:08:00+09:00" pubdate data-updated="true">Oct 19<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>前々からやる、やる、と言っていた Linux from Scratch に手をつけようと思います。</p>

<p>Linux from scratch (LFS) ていうのは、</p>

<blockquote><p>  Linux From Scratch(リナックス フロム スクラッチ、LFS)とは、Linuxを1から構築しようという試みである。すなわち、カーネルなども含めてすべて手作業でソースコードからコンパイルして作り上げる。</p>

<p>  具体的には、まず現在動作しているLinuxシステム上で、LFSをコンパイルするためのシステムを構築する。そのシステム上で、実際に稼働するLFSのシステムをコンパイルし、実際に起動することが出来る状態にまでもって行くという手順を踏む。</p>

<p>  ref. http://ja.wikipedia.org/wiki/Linux_from_Scratch</p></blockquote>

<p>ということです。でもって、</p>

<blockquote><p>  LFS teaches people how a Linux system works internally
  Building LFS teaches you about all that makes Linux tick, how things work together and depend on each other. And most importantly, how to customize it to your own tastes and needs.</p>

<p>  ref. http://www.linuxfromscratch.org/lfs/</p></blockquote>

<p>楽しく勉強できる、ということのようです。</p>

<p>本家 : <a href="http://www.linuxfromscratch.org">http://www.linuxfromscratch.org</a></p>

<h2>準備</h2>

<p><a href="http://www.linuxfromscratch.org/lfs/downloads/stable/">http://www.linuxfromscratch.org/lfs/downloads/stable/</a> から、LFS-BOOK-7.2.pdf もしくは、LFS-BOOK-7.2.tar.bz (HTMLが入ってる)をダウンロードします。このBOOKに従って粛々とビルドしていきます。</p>

<p>次に、適当なマシンにLinuxをインストールします。LFSやるなら、32bitマシンが無難ぽいです。</p>

<p>というわけで、Core 2 Quad のマシンにGentooをインストールしました。安心の最小構成です。
手元のノートからSSHでアクセスすることにします。</p>

<p>Gentooがインストールされたシステムを「ホスト」と呼びます。
ホスト側で最小限のシステムをビルドした後、新しく構築した側のシステムに移りビスドを進めていくという
流れです。</p>

<h2>パーティション作成</h2>

<p>LFSをビルドするために、新しいのパーティションを作ります。今回は10GBとしました。
スワップ領域、ブートパーティションはホストと共有することにました。</p>

<p>新しくつくったパーティションを<code>/mnt/lfs</code>以下にマウントし、そこにソースをダウンロードしてきます。</p>

<p>ダウンロードするべきソースのリストは、<a href="http://www.linuxfromscratch.org/lfs/downloads/stable/LFS-BOOK-7.2.tar.bz2">http://www.linuxfromscratch.org/lfs/downloads/stable/LFS-BOOK-7.2.tar.bz2</a> に含まれる wget-list にあります。md5sum も含まれています。</p>

<p>あとはユーザを作って、環境変数を調整したり&#8230; 詳しくはLFS-BOOK参照。</p>

<p>これで、ソースが手に入ったので、あとはひたすらビルドしてくだけなんですかね&#8230;？そうだといいですね。</p>

<p>とりあえず、今日はここまで。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/16/arel-number-2/">Arel #2</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-16T03:30:00+09:00" pubdate data-updated="true">Oct 16<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>arel-3.0.2 での以下のコードの実行過程を追い掛けています。</p>

<h2><code>Table#[]</code></h2>

<p>次に、<code>books[:title]</code> の部分について見ます。</p>

<p><code>Table::[]</code>は、<code>Arel::Attribute.new</code>を呼びだしています。</p>

<h3><code>Attribute</code> と <code>Struct</code> について</h3>

<p><code>Struct</code>とは、、、</p>

<blockquote><p>  構造体クラス。Struct.new はこのクラスのサブクラスを新たに生成します。
  個々の構造体はサブクラスから Struct.new を使って生成します。</p>

<p>  ref. <a href="http://doc.ruby-lang.org/ja/1.9.3/class/Struct.html">http://doc.ruby-lang.org/ja/1.9.3/class/Struct.html</a></p></blockquote>

<p>クラスを作るコンストラクタって何だよ&#8230;</p>

<p>arel/attributes/attribute.rb では以下のように使われています。</p>

<p><code>Arel::Predications</code> では例えば次のようなメソッドを定義しています。</p>

<ul>
<li>Arel::Predications#not_eq</li>
<li>Arel::Predications#not_eq_any</li>
<li>Arel::Predications#not_eq_all</li>
<li>Arel::Predications#eq</li>
<li>Arel::Predications#eq_any</li>
</ul>


<p>つまり、<code>Attribute</code> という構造体(のサブクラス)に対して、比較や算術演算などができるようになっている、ということです。</p>

<p><code>Table#[]</code> はこの拡張された構造体を返していることが分かります。</p>

<p>つまり、<code>books[:title].relation</code>には<code>self</code>が、<code>books[:title].name</code>には<code>:title</code>が格納されます。
さらに、<code>books[:title]</code>は<code>Predications</code>モジュールから取り込まれた<code>eq</code>というメソッドをもっています。</p>

<p>ちなみに、<code>::Arel::Attribute.new</code>としていますが、<code>Arel::Attribute</code>は<code>Arel</code>モジュールの定数で、
実体は<code>Arel::Attributes::Attribute</code>クラスです。</p>

<h2><code>Predications#eq</code></h2>

<p><code>Predications#eq</code>は↓</p>

<p><code>Nodes::Equality</code>クラスは、<code>Binary</code>を継承しています。</p>

<p><code>Binary</code>のコンストラクタは、単に引数を記憶しておくだけです。</p>

<p>この時点では、具体的な処理はまだ何もしていません。</p>

<p><code>eq</code>メソッドは<code>Arel::Nodes::Equalitiy</code>クラスのインスタンスを返します。</p>

<h3><code>Class.new</code> と <code>const_set</code></h3>

<p>話は逸れますが、binary.rbに以下のような記述がありました。</p>

<p><code>Module#const_set</code>は、モジュールに定数を定義するメソッドです。</p>

<p><code>Class.new(supercass = Object)</code>は、<code>superclass</code>を親クラスとして匿名クラスをつくります。</p>

<p>Nodeモジュールに、Binaryクラスのサブクラスを値として、各種バイナリオペレータに対応する定数を定義しています。</p>

<p>ぱっとみエグいです。</p>

<h2><code>Table#where</code></h2>

<p><code>books.where</code> を呼びだすと、新たに <code>SelectManager</code>クラスのインスタンスが生成され、
そのインスタンスが保持する<code>wheres</code>というリストに、引数が追加されます。</p>

<p><code>Expressions#eq</code>と同様、引数を記憶しておくだけです。</p>

<h2><code>SelectManager#to_sql</code></h2>

<p>さて、具体的にSQLに変換している部分を見ていくことにします。</p>

<p><code>Table#where</code> を実行すると、<code>SelectManager</code>クラスのインスタンスを返します。</p>

<p><code>SelectManager#to_sql</code> の呼び出しは、継承元の <code>TreeManager</code>クラスに渡ります。</p>

<p><code>engine.connection.visitor</code> ですが、この<code>engine</code>は最初に</p>

<p>ででてきた<code>engine</code>です。</p>

<p>デバッガで、<code>visitor</code>が現れる場所を探していくと、</p>

<p>に行きつきます。ActiveRecord側で何が起きているかは深く考えないことにして、
とにかく、<code>Arel::Visitors::SQLite</code> を見ていくことにします。</p>

<p>結局、<code>TreeManager#to_sql</code> は、<code>Arel::Visitors::ToSql#accept</code>に行き着くことになります。</p>

<p>ちなみに、<code>accept</code> の引数<code>@ast</code>は、<code>books.where</code>を呼びだした時に生成された <code>SelectManager</code>クラスのインスタンスで、これが<code>books.where</code>に渡された引数を保持しています。</p>

<h2><code>Visitors::Visitor</code></h2>

<p><code>Visitors::ToSql</code> は <code>Visitors::Visitor</code> を継承しています。</p>

<p><code>Hash.new {|hash, key| ...}</code> はハッシュのデフォルト値を定めます。<br/>
ref. <a href="http://www.ruby-lang.org/ja/old-man/html/Hash.html">http://www.ruby-lang.org/ja/old-man/html/Hash.html</a></p>

<p>上のコードで、<code>accept</code>に渡される引数は<code>Arel::Nodes::SelectStatement</code>クラスのインスタンスでした。
したがって、<code>dispatch[object.class]</code> が実行された場合、得られる値は<code>"visit_Nodes_SelectStatement"</code>となります。</p>

<p><code>visit</code>では、<code>send</code>メソッドを呼んでいます。<code>send</code>は<code>Object</code>クラスのメソッドで、
<code>obj.send(name,args)</code>は、<code>obj</code>の<code>name</code>メソッドを<code>args</code>を引数として呼びだします。</p>

<p>つまり、<code>ToSql</code>クラスの<code>visit_Nodes_SelectStatement</code>というメソッドが呼ばれることになります。</p>

<p>ちなみに、<code>Visitor</code>クラスのサブクラスには、</p>

<ul>
<li><code>DepthFirst</code></li>
<li><code>Dot</code></li>
<li><code>Informix</code></li>
<li><code>OrderClauses</code></li>
</ul>


<p>などがあります。これらのクラスのインスタンスの<code>visit</code>メソッドを呼びだしたときも、同様の処理が行われることになります。</p>

<h2><code>To_Sql#visit_Nodes_SelectStatement</code></h2>

<p><code>o.cores</code>の中に、<code>books.where</code>に渡された引数が格納されています。</p>

<p>ここで察しがつくかと思いますが、ここから<code>visit</code>を再帰的に呼び出していきます。</p>

<p><code>o.wheres</code> に <code>books.where</code> に渡された引数 <code>books[:title].eq(...)</code> が格納されています。
<code>books[:title].eq(...)</code>のクラスは<code>Arel::Nodes::Equality</code>なので、8行目で<code>visit_Arel_Nodes_Equality</code>が呼ばれます。</p>

<p><code>visit o.left</code>の部分で、<code>books[:title]</code>に対して<code>visit</code>が呼ばれます。</p>

<p>最終的に、<code>"SELECT FORM books WHERE books.title = 'Head First Rails'"</code>という文字列が得られるはずです。</p>

<h2>まとめ</h2>

<p>Arelに特徴的なのは、<code>#where</code> や <code>#eq</code> というメソッドが呼ばれたときに、
特に具体的な変換は行わず、引数や<code>self</code>を保持するインスタンスを返すだけ、ということではないでしょうか。</p>

<p>いってみれば、<code>#where</code>や<code>#eq</code>などを使いながら「SQL構文木」を構築していくのだと思います。</p>

<p><code>#to_sql</code>が呼ばれたときに、この「構文木」のノードや葉を対応するSQLに変換していくわけです。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/15/arel-number-1/">Arel #1</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-15T00:00:00+09:00" pubdate data-updated="true">Oct 15<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Arelとは、SQLクエリを構築するためのrubyのライブラリです。</p>

<p>ActiveRecord(モデル層)とデータベースの間に立ち、
ActiveRecordにおけるメソッド呼び出しをSQLクエリに変換してくれます。<br/>
ref. <a href="http://gihyo.jp/dev/serial/01/ruby/0043">http://gihyo.jp/dev/serial/01/ruby/0043</a></p>

<p>arel-3.0.2、activerecord-3.2.8 を使ってます。</p>

<p>目標は、次のコードの実行過程を理解すること。</p>

<pre><code>books = Arel::Table.new :books
books = books.where(books[:title].eq('Head First Rails'))
puts books.to_sql
</code></pre>

<p>このコードを実行すると、<code>SELECT FROM books WHERE books.title = 'Head First Rails'</code>という文字列が返ってきます。</p>

<hr />

<p>Arel を使うためには、データベースとのコネクションを作る必要があるのですが、
その部分は ActiveRecord のおまかせします。</p>

<pre><code>require 'arel'
require 'sqlite3'
require 'active_record'

ActiveRecord::Base.configurations = {'development' =&gt; {:adapter =&gt; 'sqlite3', :database =&gt; 'books.sqlite3'}}
ActiveRecord::Base.establish_connection('development')

Arel::Table.engine = ActiveRecord::Base
</code></pre>

<hr />

<p>さて、つらつらと読んでいくことにします。</p>

<p>でも、眠いので今日はここまで。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/14/rails-sourcecode-reading-number-3/">Rails Sourcecode Reading #3 &#8211; 準備</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-14T23:20:00+09:00" pubdate data-updated="true">Oct 14<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>まずは、ソースコードを用意しました。
bundle を利用することにしました。</p>

<pre><code>$ mkdir rails_sourcecode_reading
$ cd rails_sourcecode_reading
$ mkdir src

$ vi Gemfile

-----------------------------
# Gemfile
source "http://rubygems.org"

gem "rails", "3.2.8"
-----------------------------

$ bundle install --path="./src"
</code></pre>

<p>これで、src/ruby/1.9.1/gems 以下に、railsに関連するgemたちが全部入ります。</p>

<p>次に、ソースコードを読む環境を整えました。
もちろん、emacsで読んでいきます。
以下は、利用した elisp とか gem とかです。</p>

<ul>
<li><p>ctags</p>

<p><code>ctags -e -R . --langmap=Ruby:.rb --ruby-types=cfFm</code> でemacs用のタグファイルを生成し、
<code>M-.</code> でタグジャンプできます。</p>

<p>参考 : <a href="http://wiki.livedoor.jp/koziy/d/ruby/ctags">http://wiki.livedoor.jp/koziy/d/ruby/ctags</a></p></li>
<li><p>rdefs (gem)</p></li>
<li><p>rinari (elisp)</p>

<p>packege-install ではいります。
rails 用の統合開発環境みたいなかんじです。</p></li>
<li><p>ruby-debug</p>

<p><a href="http://jampin.blog20.fc2.com/blog-entry-18.html">http://jampin.blog20.fc2.com/blog-entry-18.html</a> を参考に。</p></li>
<li><p>yard</p>

<p><code>yard server</code> でWebサーバ起動</p></li>
</ul>


<p>まずは、ActiveRecord と Arel あたりから読んでいこうと思います。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/14/rails-sourcecode-reading-number-2/">Rails Sourcecode Reading #2</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-14T23:20:00+09:00" pubdate data-updated="true">Oct 14<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>ruby の勉強と Webアプリケーションフレームワークの勉強のために、
Rails のソースコードをじっくり読んでいこう、と思い立ちました。</p>

<p>そろそろまじめにやろうと思います。</p>

<p>勉強したことは、このページに書いていきます。</p>

<p>ruby のバージョンは 1.9.3p194、rails のバージョンは 3.2.8 です。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/14/arel-number-1/">Arel #1</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-14T23:20:00+09:00" pubdate data-updated="true">Oct 14<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content">
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/07/15/dzen-with-japanese/">Dzenで日本語</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-15T04:38:00+09:00" pubdate data-updated="true">Jul 15<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>dzen で日本語を表示するまでのまとめ。</p>

<p>portage で提供されている dzen をインストールしたところ、日本語が化けた。</p>

<p>どうやら、xft のサポートをつける必要があるみたい。</p>

<p>ところが、通常のレポジトリで提供されているソースは xft に対応していないようだ。</p>

<p>というわけで、<a href="https://github.com/robm/dzen">https://github.com/robm/dzen</a> から最新版をもってきて、普通に make してインストールしたら、表示できました。</p>

<p>ちなみに、フォントを指定してやらないと、かなり汚い。アンチエイリアスも設定できた。</p>

<pre><code>dzen2 -fn 'xft\
          \:IPAGothic\
          \:pixelsize=13\
          \:weight=regular\
          \:width=semicondensed\
          \:dpi=96\
          \:hinting=true\
          \:hintstyle=hintslight\
          \:antialias=true\
          \:rgba=rgb\
          \:lcdfilter=lcdlight\
          \'
</code></pre>

<p>一応ebuildをのっけてみる。。。</p>

<p><a href="https://github.com/ojima-h/my-overlays/blob/master/x11-misc/dzen/dzen-9999.ebuild">https://github.com/ojima-h/my-overlays/blob/master/x11-misc/dzen/dzen-9999.ebuild</a></p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/08/30/hive-optimization/">Hive クエリを最適化する</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/08/29/hadoop-tuning/">Hadoop のチューニングに関してまとめ</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/26/activerecord-number-1/">ActiveRecord #1</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/19/linux-from-scratch-number-0/">Linux from Scratch 始めます</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/16/arel-number-2/">arel #2</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>
    <ul id="category-list"><li><a href='/blog/categories/emacs'>emacs (2)</a></li><li><a href='/blog/categories/gentoo'>gentoo (2)</a></li><li><a href='/blog/categories/hadoop'>Hadoop (1)</a></li><li><a href='/blog/categories/hive'>Hive (1)</a></li><li><a href='/blog/categories/inov2013'>inov2013 (6)</a></li><li><a href='/blog/categories/linux-from-scratch'>linux-from-scratch (1)</a></li><li><a href='/blog/categories/misc'>misc (2)</a></li><li><a href='/blog/categories/octpress'>octpress (1)</a></li><li><a href='/blog/categories/rails'>rails (1)</a></li><li><a href='/blog/categories/rails-sourcecode-reading'>rails-sourcecode-reading (7)</a></li><li><a href='/blog/categories/ruby'>ruby (1)</a></li><li><a href='/blog/categories/xmonad'>xmonad (1)</a></li></ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/ojima-h">@ojima-h</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'ojima-h',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("ojima_h", 4, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/ojima_h" class="twitter-follow-button" data-show-count="false">Follow @ojima_h</a>
  
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - ojima_h -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
